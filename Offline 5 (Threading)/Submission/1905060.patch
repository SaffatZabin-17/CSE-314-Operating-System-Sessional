diff --git a/Makefile b/Makefile
index 39a99d7..a822569 100644
--- a/Makefile
+++ b/Makefile
@@ -132,6 +132,8 @@ UPROGS=\
 	$U/_grind\
 	$U/_wc\
 	$U/_zombie\
+	$U/_threads\
+	$U/_producer_consumer\
 
 fs.img: mkfs/mkfs README $(UPROGS)
 	mkfs/mkfs fs.img README $(UPROGS)
diff --git a/kernel/defs.h b/kernel/defs.h
index a3c962b..d40ce8d 100644
--- a/kernel/defs.h
+++ b/kernel/defs.h
@@ -85,10 +85,14 @@ void            printfinit(void);
 int             cpuid(void);
 void            exit(int);
 int             fork(void);
+int             thread_create(void (*)(void *), void *, void *);
+int             thread_join(int);
+void            thread_exit(void);
 int             growproc(int);
 void            proc_mapstacks(pagetable_t);
 pagetable_t     proc_pagetable(struct proc *);
 void            proc_freepagetable(pagetable_t, uint64);
+void            thread_freepagetable(pagetable_t, uint64);
 int             kill(int);
 int             killed(struct proc*);
 void            setkilled(struct proc*);
@@ -99,6 +103,8 @@ void            procinit(void);
 void            scheduler(void) __attribute__((noreturn));
 void            sched(void);
 void            sleep(void*, struct spinlock*);
+void            thread_sleep(uint64);
+void            thread_wakeup(int);  
 void            userinit(void);
 int             wait(uint64);
 void            wakeup(void*);
@@ -165,12 +171,16 @@ void            uvmfirst(pagetable_t, uchar *, uint);
 uint64          uvmalloc(pagetable_t, uint64, uint64, int);
 uint64          uvmdealloc(pagetable_t, uint64, uint64);
 int             uvmcopy(pagetable_t, pagetable_t, uint64);
+int             uvmmirror(pagetable_t, pagetable_t, uint64);
+int             uvmrangemirror(pagetable_t, pagetable_t, uint64, uint64);
 void            uvmfree(pagetable_t, uint64);
+void            uvmfree_thread(pagetable_t, uint64);
 void            uvmunmap(pagetable_t, uint64, uint64, int);
 void            uvmclear(pagetable_t, uint64);
 pte_t *         walk(pagetable_t, uint64, int);
 uint64          walkaddr(pagetable_t, uint64);
 int             copyout(pagetable_t, uint64, char *, uint64);
+int             copyout_threadlock(pagetable_t, uint64);
 int             copyin(pagetable_t, char *, uint64, uint64);
 int             copyinstr(pagetable_t, char *, uint64, uint64);
 
diff --git a/kernel/proc.c b/kernel/proc.c
index 959b778..0124380 100644
--- a/kernel/proc.c
+++ b/kernel/proc.c
@@ -13,7 +13,10 @@ struct proc proc[NPROC];
 struct proc *initproc;
 
 int nextpid = 1;
+int next_memid = 1;
+
 struct spinlock pid_lock;
+struct spinlock memid_lock;
 
 extern void forkret(void);
 static void freeproc(struct proc *p);
@@ -51,10 +54,13 @@ procinit(void)
   
   initlock(&pid_lock, "nextpid");
   initlock(&wait_lock, "wait_lock");
+  initlock(&memid_lock, "next_memid");
   for(p = proc; p < &proc[NPROC]; p++) {
       initlock(&p->lock, "proc");
       p->state = UNUSED;
       p->kstack = KSTACK((int) (p - proc));
+      p->memlock = (struct spinlock*) kalloc();       //Allocate memory lock for each process
+      initlock(p->memlock, "memlock");                //Initialize memory lock
   }
 }
 
@@ -102,6 +108,19 @@ allocpid()
   return pid;
 }
 
+int
+allocmemid()
+{
+  int memid;
+
+  acquire(&pid_lock);
+  memid = next_memid;
+  next_memid = next_memid + 1;
+  release(&pid_lock);
+
+  return memid;
+}
+
 // Look in the process table for an UNUSED proc.
 // If found, initialize state required to run in the kernel,
 // and return with p->lock held.
@@ -124,6 +143,8 @@ allocproc(void)
 found:
   p->pid = allocpid();
   p->state = USED;
+  p->mem_id = allocmemid();
+  p->is_thread = 0;
 
   // Allocate a trapframe page.
   if((p->trapframe = (struct trapframe *)kalloc()) == 0){
@@ -158,12 +179,20 @@ freeproc(struct proc *p)
   if(p->trapframe)
     kfree((void*)p->trapframe);
   p->trapframe = 0;
-  if(p->pagetable)
-    proc_freepagetable(p->pagetable, p->sz);
+  if(p->is_thread){
+    if(p->pagetable){
+      thread_freepagetable(p->pagetable, p->sz);
+    }
+  }
+  else{
+    if(p->pagetable)
+      proc_freepagetable(p->pagetable, p->sz);
+  }
   p->pagetable = 0;
   p->sz = 0;
   p->pid = 0;
   p->parent = 0;
+  p->mem_id = 0;
   p->name[0] = 0;
   p->chan = 0;
   p->killed = 0;
@@ -215,6 +244,13 @@ proc_freepagetable(pagetable_t pagetable, uint64 sz)
   uvmfree(pagetable, sz);
 }
 
+void
+thread_freepagetable(pagetable_t pagetable, uint64 sz){
+  uvmunmap(pagetable, TRAMPOLINE, 1, 0);
+  uvmunmap(pagetable, TRAPFRAME, 1, 0);
+  uvmfree_thread(pagetable, sz);
+}
+
 // a user program that calls exec("/init")
 // assembled from ../user/initcode.S
 // od -t xC ../user/initcode
@@ -259,10 +295,13 @@ userinit(void)
 int
 growproc(int n)
 {
-  uint64 sz;
+  uint64 sz, old_sz;
   struct proc *p = myproc();
 
+  //acquire(p->memlock);
+
   sz = p->sz;
+  old_sz = p->sz;
   if(n > 0){
     if((sz = uvmalloc(p->pagetable, sz, sz + n, PTE_W)) == 0) {
       return -1;
@@ -271,6 +310,23 @@ growproc(int n)
     sz = uvmdealloc(p->pagetable, sz, sz + n);
   }
   p->sz = sz;
+
+  struct proc *pp;
+  for(pp = proc; pp < &proc[NPROC]; pp++){
+    if(pp != p && pp->mem_id == p->mem_id){
+      if(n >= 0){
+        if(uvmrangemirror(p->pagetable, pp->pagetable, old_sz, sz) < 0){
+          return -1;
+        }
+      }
+      else if(n < 0){
+        uvmunmap(pp->pagetable, PGROUNDUP(sz), ((PGROUNDUP(old_sz) - PGROUNDUP(sz))/PGSIZE), 0);
+      }
+      p->sz = sz;
+    }
+  }
+
+  //release(p->memlock);
   return 0;
 }
 
@@ -302,6 +358,65 @@ fork(void)
   // Cause fork to return 0 in the child.
   np->trapframe->a0 = 0;
 
+  np->is_thread = p->is_thread;
+
+  // increment reference counts on open file descriptors.
+  for(i = 0; i < NOFILE; i++)
+    if(p->ofile[i])
+      np->ofile[i] = filedup(p->ofile[i]);
+  np->cwd = idup(p->cwd);
+
+  safestrcpy(np->name, p->name, sizeof(p->name));
+
+  pid = np->pid;
+
+  release(&np->lock);
+
+  acquire(&wait_lock);
+  np->parent = p;
+  release(&wait_lock);
+
+  acquire(&np->lock);
+  np->state = RUNNABLE;
+  release(&np->lock);
+
+  return pid;
+}
+
+int
+thread_create(void (*fcn)(void*), void *arg, void *stack)
+{
+  int i, pid;
+  struct proc *np;
+  struct proc *p = myproc();
+
+  // Allocate process.
+  if((np = allocproc()) == 0){
+    return -1;
+  }
+
+  // Copy user memory from parent to child.
+  if(uvmmirror(p->pagetable, np->pagetable, p->sz) < 0){
+    freeproc(np);
+    release(&np->lock);
+    return -1;
+  }
+  np->sz = p->sz;
+
+  // copy saved user registers.
+  *(np->trapframe) = *(p->trapframe);
+
+  np->trapframe->epc = (uint64)fcn;
+  np->trapframe->a0 = (uint64)arg;
+  np->trapframe->sp = (uint64)stack + PGSIZE - sizeof(void *);
+
+  np->is_thread = 1;
+
+  np->mem_id = p->mem_id;
+
+  // Cause fork to return 0 in the child.
+  //np->trapframe->a0 = 0;
+
   // increment reference counts on open file descriptors.
   for(i = 0; i < NOFILE; i++)
     if(p->ofile[i])
@@ -325,6 +440,7 @@ fork(void)
   return pid;
 }
 
+
 // Pass p's abandoned children to init.
 // Caller must hold wait_lock.
 void
@@ -385,6 +501,48 @@ exit(int status)
   panic("zombie exit");
 }
 
+void
+thread_exit(void)
+{
+  struct proc *p = myproc();
+
+  if(p == initproc)
+    panic("init exiting");
+
+  // Close all open files.
+  for(int fd = 0; fd < NOFILE; fd++){
+    if(p->ofile[fd]){
+      struct file *f = p->ofile[fd];
+      fileclose(f);
+      p->ofile[fd] = 0;
+    }
+  }
+
+  begin_op();
+  iput(p->cwd);
+  end_op();
+  p->cwd = 0;
+
+  acquire(&wait_lock);
+
+  // Give any children to init.
+  reparent(p);
+
+  // Parent might be sleeping in wait().
+  wakeup(p->parent);
+
+  acquire(&p->lock);
+
+  p->xstate = 0;
+  p->state = ZOMBIE;
+
+  release(&wait_lock);
+
+  // Jump into the scheduler, never to return.
+  sched();
+  panic("zombie exit");
+}
+
 // Wait for a child process to exit and return its pid.
 // Return -1 if this process has no children.
 int
@@ -434,6 +592,91 @@ wait(uint64 addr)
   }
 }
 
+int
+thread_join(int thread_id)
+{
+  // struct proc *pp;
+  // int havekids, pid;
+  // struct proc *p = myproc();
+
+  // acquire(&wait_lock);
+
+  // for(;;){
+  //   // Scan through table looking for exited children.
+  //   havekids = 0;
+  //   for(pp = proc; pp < &proc[NPROC]; pp++){
+  //     if(pp->parent == p){
+
+  //       pp->pid = thread_id;
+  //       // make sure the child isn't still in exit() or swtch().
+  //       acquire(&pp->lock);
+
+  //       havekids = 1;
+  //       if(pp->state == ZOMBIE){
+  //         // Found one.
+  //         pid = pp->pid;
+  //         freeproc(pp);
+  //         release(&pp->lock);
+  //         release(&wait_lock);
+  //         return pid;
+  //       }
+  //       release(&pp->lock);
+  //     }
+  //   }
+
+  //   // No point waiting if we don't have any children.
+  //   if(!havekids || killed(p)){
+  //     release(&wait_lock);
+  //     return -1;
+  //   }
+
+  //   // Wait for a child to exit.
+  //   sleep(p, &wait_lock);  //DOC: wait-sleep
+  // }
+
+  struct proc *p;
+  int havekids;
+  struct proc *cp = myproc();
+
+  acquire(&wait_lock);
+
+  for(;;){
+    // Scan through table looking for exited children.
+    havekids = 0;
+    for(p = proc; p < &proc[NPROC]; p++){
+      if(p != cp && p->parent == cp && p->is_thread == 1){
+        // make sure the child isn't still in exit() or swtch().
+        acquire(&p->lock);
+        //acquire(&p->mem_lock[p->mem_id]);
+
+        havekids = 1;
+        if(p->pid == thread_id){
+          // Found one.
+          if(p->state == ZOMBIE){
+            // Found one.
+            freeproc(p);
+            release(&p->lock);
+            release(&wait_lock);
+            return thread_id;
+          }
+        }
+        //release(&p->mem_lock[p->mem_id]);
+        release(&p->lock);
+      }
+    }
+
+    // No point waiting if we don't have any children.
+    if(!havekids){
+      release(&wait_lock);
+      return -1;
+    }
+
+    // Wait for a child to exit.
+    sleep(cp, &wait_lock);  //DOC: wait-sleep
+  }
+}
+
+
 // Per-CPU process scheduler.
 // Each CPU calls scheduler() after setting itself up.
 // Scheduler never returns.  It loops, doing:
@@ -561,6 +804,31 @@ sleep(void *chan, struct spinlock *lk)
   acquire(lk);
 }
 
+void
+thread_sleep(uint64 locked){
+  struct proc *p = myproc();
+  
+  // Must acquire p->lock in order to
+  // change p->state and then call sched.
+  // Once we hold p->lock, we can be
+  // guaranteed that we won't miss any wakeup
+  // (wakeup locks p->lock),
+  // so it's okay to release lk.
+
+  acquire(&p->lock);  //DOC: sleeplock1
+  
+  copyout_threadlock(p->pagetable, locked);
+
+  // Go to sleep.
+  p->state = SLEEPING;
+
+  sched();
+
+  // Tidy up.
+
+  // Reacquire original lock.
+  release(&p->lock);
+}
 // Wake up all processes sleeping on chan.
 // Must be called without any p->lock.
 void
@@ -579,6 +847,21 @@ wakeup(void *chan)
   }
 }
 
+
+void
+thread_wakeup(int thread_id){
+  struct proc *p;
+
+  for(p = proc; p < &proc[NPROC]; p++) {
+    if(p->pid == thread_id){
+      acquire(&p->lock);
+      if(p->state == SLEEPING) {
+        p->state = RUNNABLE;
+      }
+      release(&p->lock);
+    }
+  }
+}
 // Kill the process with the given pid.
 // The victim won't exit until it tries to return
 // to user space (see usertrap() in trap.c).
diff --git a/kernel/proc.h b/kernel/proc.h
index d021857..5f64821 100644
--- a/kernel/proc.h
+++ b/kernel/proc.h
@@ -104,4 +104,7 @@ struct proc {
   struct file *ofile[NOFILE];  // Open files
   struct inode *cwd;           // Current directory
   char name[16];               // Process name (debugging)
+  struct spinlock* memlock;    // find places to set and release the locks
+  int is_thread;               // 1 if thread, 0 if process
+  int mem_id;                  // All threads will have the same physical pages with the mothrer, hence the same memory ID
 };
diff --git a/kernel/syscall.c b/kernel/syscall.c
index ed65409..a88312e 100644
--- a/kernel/syscall.c
+++ b/kernel/syscall.c
@@ -101,6 +101,11 @@ extern uint64 sys_unlink(void);
 extern uint64 sys_link(void);
 extern uint64 sys_mkdir(void);
 extern uint64 sys_close(void);
+extern uint64 sys_thread_create(void);
+extern uint64 sys_thread_join(void);
+extern uint64 sys_thread_exit(void);
+extern uint64 sys_thread_sleep(void);
+extern uint64 sys_thread_wakeup(void);
 
 // An array mapping syscall numbers from syscall.h
 // to the function that handles the system call.
@@ -126,6 +131,11 @@ static uint64 (*syscalls[])(void) = {
 [SYS_link]    sys_link,
 [SYS_mkdir]   sys_mkdir,
 [SYS_close]   sys_close,
+[SYS_thread_create] sys_thread_create,
+[SYS_thread_join] sys_thread_join,
+[SYS_thread_exit] sys_thread_exit,
+[SYS_thread_sleep] sys_thread_sleep,
+[SYS_thread_wakeup] sys_thread_wakeup,
 };
 
 void
diff --git a/kernel/syscall.h b/kernel/syscall.h
index bc5f356..a7ef698 100644
--- a/kernel/syscall.h
+++ b/kernel/syscall.h
@@ -20,3 +20,8 @@
 #define SYS_link   19
 #define SYS_mkdir  20
 #define SYS_close  21
+#define SYS_thread_create 22
+#define SYS_thread_join 23
+#define SYS_thread_exit 24
+#define SYS_thread_sleep 25
+#define SYS_thread_wakeup 26
\ No newline at end of file
diff --git a/kernel/sysproc.c b/kernel/sysproc.c
index 1de184e..9ee97a6 100644
--- a/kernel/sysproc.c
+++ b/kernel/sysproc.c
@@ -27,6 +27,31 @@ sys_fork(void)
   return fork();
 }
 
+uint64
+sys_thread_create(void){
+  uint64 fcn, arg, stack;
+
+  argaddr(0, &fcn);
+  argaddr(1, &arg);
+  argaddr(2, &stack);
+
+  return thread_create((void*)fcn, (void*)arg, (void*)stack);
+}
+
+uint64
+sys_thread_join(void){
+  int thread_id;
+  argint(0, &thread_id);
+  return thread_join(thread_id);
+}
+
+uint64
+sys_thread_exit(void){
+  thread_exit();
+  return 0;
+}
+
+
 uint64
 sys_wait(void)
 {
@@ -68,6 +93,28 @@ sys_sleep(void)
   return 0;
 }
 
+uint64
+sys_thread_sleep(void){
+  uint64 locked;
+
+  argaddr(0, &locked);
+
+  thread_sleep(locked);
+
+  return 0;
+}
+
+uint64
+sys_thread_wakeup(void){
+  int thread_id;
+
+  argint(0, &thread_id);
+
+  thread_wakeup(thread_id);
+
+  return 0;
+}
+
 uint64
 sys_kill(void)
 {
diff --git a/kernel/vm.c b/kernel/vm.c
index 9f69783..46152ba 100644
--- a/kernel/vm.c
+++ b/kernel/vm.c
@@ -296,6 +296,13 @@ uvmfree(pagetable_t pagetable, uint64 sz)
   freewalk(pagetable);
 }
 
+void
+uvmfree_thread(pagetable_t pagetable, uint64 sz){
+  if(sz > 0)
+    uvmunmap(pagetable, 0, PGROUNDUP(sz)/PGSIZE, 0);
+  freewalk(pagetable);
+}
+
 // Given a parent process's page table, copy
 // its memory into a child's page table.
 // Copies both the page table and the
@@ -332,6 +339,58 @@ uvmcopy(pagetable_t old, pagetable_t new, uint64 sz)
   return -1;
 }
 
+int
+uvmmirror(pagetable_t old, pagetable_t new, uint64 sz){
+  pte_t *pte;
+  uint64 pa, i;
+  uint flags;
+
+  for(i = 0; i < sz; i += PGSIZE){
+    if((pte = walk(old, i, 0)) == 0)
+      panic("uvmcopy: pte should exist");
+    if((*pte & PTE_V) == 0)
+      panic("uvmcopy: page not present");
+    pa = PTE2PA(*pte);
+    flags = PTE_FLAGS(*pte);
+
+    if(mappages(new, i, PGSIZE, (uint64)pa, flags) != 0){
+      //kfree((void*)pa);
+      goto err;
+    }
+  }
+  return 0;
+
+ err:
+  uvmunmap(new, 0, i / PGSIZE, 1);
+  return -1;
+}
+
+int
+uvmrangemirror(pagetable_t old, pagetable_t new, uint64 sz, uint64 oldsz){
+  pte_t *pte;
+  uint64 pa, i;
+  uint flags;
+
+  for(i = PGROUNDUP(oldsz); i < sz; i += PGSIZE){
+    if((pte = walk(old, i, 0)) == 0)
+      panic("uvmcopy: pte should exist");
+    if((*pte & PTE_V) == 0)
+      panic("uvmcopy: page not present");
+    pa = PTE2PA(*pte);
+    flags = PTE_FLAGS(*pte);
+
+    if(mappages(new, i, PGSIZE, (uint64)pa, flags) != 0){
+      //kfree((void*)pa);
+      goto err;
+    }
+  }
+  return 0;
+
+ err:
+  uvmunmap(new, 0, i / PGSIZE, 1);
+  return -1;
+}
+
 // mark a PTE invalid for user access.
 // used by exec for the user stack guard page.
 void
@@ -370,6 +429,29 @@ copyout(pagetable_t pagetable, uint64 dstva, char *src, uint64 len)
   return 0;
 }
 
+int
+copyout_threadlock(pagetable_t pagetable, uint64 dstva){
+  uint64 va0, pa0;
+
+  va0 = PGROUNDDOWN(dstva);
+  pa0 = walkaddr(pagetable, va0);
+  if(pa0 == 0)
+    return -1;
+  // n = PGSIZE - (dstva - va0);
+  // if(n > len)
+  //   n = len;
+  // memmove((void *)(pa0 + (dstva - va0)), src, n);
+
+  // len -= n;
+  // src += n;
+  //dstva = va0 + PGSIZE;
+
+  __sync_synchronize();
+
+  __sync_lock_release((uint8*)(pa0 + (dstva - va0)));
+  return 0;
+}
+
 // Copy from user to kernel.
 // Copy len bytes to dst from virtual address srcva in a given page table.
 // Return 0 on success, -1 on error.
diff --git a/user/producer_consumer.c b/user/producer_consumer.c
new file mode 100644
index 0000000..f406c57
--- /dev/null
+++ b/user/producer_consumer.c
@@ -0,0 +1,145 @@
+#include "kernel/types.h"
+#include "user/user.h"
+#include "user/semaphore.c"
+
+struct producer_consumer_queue
+{
+	int arr[16];
+	int front;
+	int rear;
+	int size;
+};
+
+struct producer_consumer_queue que;
+
+void producer_consumer_queue()
+{
+	que.front = 0;
+	que.rear = 0;
+	que.size = 0;
+}
+
+void push_producer(int x)
+{
+	que.arr[que.rear] = x;
+	que.rear = (que.rear + 1) % 16;
+	que.size++;
+}
+
+int front_queue()
+{
+	if (que.size == 0)
+		return -1;
+	return que.arr[que.front];
+}
+
+void pop_consumer()
+{
+	que.front = (que.front + 1) % 16;
+	que.size--;
+}
+// a mutex object lock
+struct thread_mutex lock;
+// a semaphore object empty
+struct semaphore empty;
+// a semaphore object full
+struct semaphore full;
+
+void init_semaphore()
+{
+	// initialize mutex lock
+	thread_mutex_init(&lock, "sem_lock");
+	// initialize semaphore empty with 5
+	sem_init(&empty, 5);
+	// initialize semaphore full with 0
+	sem_init(&full, 0);
+}
+
+void ProducerFunc(void *arg)
+{
+	thread_mutex_lock(&lock);
+	printf("%s\n", (char *)arg);
+	thread_mutex_unlock(&lock);
+
+	int i;
+	for (i = 1; i <= 10; i++)
+	{
+		// wait for semphore empty
+		sem_wait(&empty);
+		// wait for mutex lock
+		thread_mutex_lock(&lock);
+
+		sleep(1);
+		push_producer(i);
+		printf("producer produced item %d\n", i);
+
+		// unlock mutex lock
+		thread_mutex_unlock(&lock);
+		// post semaphore full
+		sem_post(&full);
+	}
+
+	thread_exit();
+
+	return;
+}
+
+void ConsumerFunc(void *arg)
+{
+	thread_mutex_lock(&lock);
+	printf("%s\n", (char *)arg);
+	thread_mutex_unlock(&lock);
+
+	int i;
+	for (i = 1; i <= 10; i++)
+	{
+		// wait for semphore full
+
+		sem_wait(&full);
+		// wait for mutex lock
+		thread_mutex_lock(&lock);
+
+		sleep(1);
+		int item = front_queue();
+
+		pop_consumer();
+
+		printf("consumer consumed item %d\n", item);
+
+		// unlock mutex lock
+		thread_mutex_unlock(&lock);
+
+		// post semaphore empty
+		sem_post(&empty);
+	}
+
+	thread_exit();
+
+	return;
+}
+
+int main(void)
+{
+
+	init_semaphore();
+
+	char *message1 = "i am producer";
+	char *message2 = "i am consumer";
+
+	void *s1, *s2;
+	int thread1, thread2, r1, r2;
+
+	s1 = malloc(4096);
+	s2 = malloc(4096);
+
+	thread1 = thread_create(ProducerFunc, (void *)message1, s1);
+	thread2 = thread_create(ConsumerFunc, (void *)message2, s2);
+
+	r1 = thread_join(thread1);
+	r2 = thread_join(thread2);
+
+	printf("r1 = %d\n", r1);
+	printf("r2 = %d\n", r2);
+
+	exit(0);
+}
diff --git a/user/semaphore.c b/user/semaphore.c
new file mode 100644
index 0000000..7d46123
--- /dev/null
+++ b/user/semaphore.c
@@ -0,0 +1,38 @@
+#include "user/semaphore.h"
+#include "user/user.h"
+#include "kernel/types.h"
+//#include "user/thread_conditional.h"
+//#include "user/thread_mutex_lock.c"
+//#include "user/thread_conditional.c"
+
+
+int sem_init(struct semaphore *sem, int value){
+    sem->value = value;
+    thread_mutex_init(&sem->mutex_lock, "sem_mutex");
+    thread_cond_init(&sem->cond);
+    return 0;
+}
+
+void sem_wait(struct semaphore *sem){
+    thread_mutex_lock(&sem->mutex_lock);
+   
+    while(sem->value <= 0){
+        thread_cond_wait(&sem->cond, &sem->mutex_lock);
+    }
+
+    sem->value--;
+
+    thread_mutex_unlock(&sem->mutex_lock);
+    return;
+}
+
+void sem_post(struct semaphore *sem){
+    thread_mutex_lock(&sem->mutex_lock);
+
+    sem->value++;
+
+    thread_cond_signal(&sem->cond);
+
+    thread_mutex_unlock(&sem->mutex_lock);
+    return;
+}
\ No newline at end of file
diff --git a/user/semaphore.h b/user/semaphore.h
new file mode 100644
index 0000000..fbba51c
--- /dev/null
+++ b/user/semaphore.h
@@ -0,0 +1,8 @@
+//#include "user/thread_mutex_lock.h"
+#include "user/thread_conditional.c"
+
+struct semaphore{
+    int value;
+    struct thread_mutex mutex_lock;
+    struct thread_cond cond;
+};
\ No newline at end of file
diff --git a/user/thread_conditional.c b/user/thread_conditional.c
new file mode 100644
index 0000000..cff9ab7
--- /dev/null
+++ b/user/thread_conditional.c
@@ -0,0 +1,46 @@
+//#include "user/thread_mutex_lock.h"
+#include "user/thread_conditional.h"
+//#include "user/thread_mutex_lock.c"
+#include "user/user.h"
+#include "kernel/types.h"
+
+
+void thread_cond_init(struct thread_cond *cond){
+    thread_mutex_init(&cond->mutex_lock, "cond_mutex");
+    thread_mutex_init(&cond->lock_list, "lock_list");
+    //cond->que->queue();
+    queue(&cond->que);
+}
+
+void thread_cond_wait(struct thread_cond *cond, struct thread_mutex *lock){
+    
+    thread_mutex_lock(&cond->lock_list);
+    
+    push(&cond->que, getpid());
+    
+    thread_mutex_unlock(&cond->lock_list);
+    
+    thread_sleep(&lock->locked);
+    
+    thread_mutex_lock(lock);
+
+    return;
+}
+
+void thread_cond_signal(struct thread_cond *cond){
+    thread_mutex_lock(&cond->lock_list);
+
+    if(is_empty(&cond->que)){
+        thread_mutex_unlock(&cond->lock_list);
+        return;
+    }
+    int pid = front(&cond->que);
+
+    pop(&cond->que);
+
+    thread_mutex_unlock(&cond->lock_list);
+
+    thread_wakeup(pid);
+
+    return;
+}
\ No newline at end of file
diff --git a/user/thread_conditional.h b/user/thread_conditional.h
new file mode 100644
index 0000000..9007a22
--- /dev/null
+++ b/user/thread_conditional.h
@@ -0,0 +1,51 @@
+#include "user/thread_mutex_lock.c"
+//#include "user/thread_mutex_lock.h"
+
+struct queue
+{
+	int arr[16];
+	int front;
+	int rear;
+	int size;
+};
+
+void queue(struct queue *que)
+{
+	que->front = 0;
+	que->rear = 0;
+	que->size = 0;
+}
+
+void push(struct queue *que, int x)
+{
+	que->arr[que->rear] = x;
+	que->rear = (que->rear + 1) % 16;
+	que->size++;
+}
+
+int front(struct queue *que)
+{
+	if (que->size == 0)
+		return -1;
+	return que->arr[que->front];
+}
+
+void pop(struct queue *que)
+{
+	que->front = (que->front + 1) % 16;
+	que->size--;
+}
+
+int is_empty(struct queue *que)
+{
+	if(que->size == 0)
+		return 1;
+	return 0;
+}
+
+struct thread_cond
+{
+	struct queue que;				// Stores pid for waiting threads
+	struct thread_mutex mutex_lock; // Mutex lock for conditional variable
+	struct thread_mutex lock_list;	// Mutex lock for list of conditional variables
+};
\ No newline at end of file
diff --git a/user/thread_mutex_lock.c b/user/thread_mutex_lock.c
new file mode 100644
index 0000000..4bc4c37
--- /dev/null
+++ b/user/thread_mutex_lock.c
@@ -0,0 +1,66 @@
+#include "user/thread_mutex_lock.h"
+
+void
+thread_mutex_init(struct thread_mutex *lk, char *name)
+{
+  lk->name = name;
+  lk->locked = 0;
+  lk->pid = 0;
+}
+
+// Acquire the lock.
+// Loops (spins) until the lock is acquired.
+void
+thread_mutex_lock(struct thread_mutex *lk)
+{
+
+  // On RISC-V, sync_lock_test_and_set turns into an atomic swap:
+  //   a5 = 1
+  //   s1 = &lk->locked
+  //   amoswap.w.aq a5, a5, (s1)
+  while(__sync_lock_test_and_set(&lk->locked, 1) != 0){
+    sleep(1);
+  };
+
+  // Tell the C compiler thread_spin_lock(&lock);
+  //and the processor to not move loads or stores
+  // past this point, to ensure that the critical section's memory
+  // references happen strictly after the lock is acquired.
+  // On RISC-V, this emits a fence instruction.
+  __sync_synchronize();
+
+  lk->pid = getpid();
+}
+
+// Release the lock.
+void
+thread_mutex_unlock(struct thread_mutex *lk)
+{
+
+  lk -> pid = 0;
+  // Tell the C compiler and the CPU to not move loads or stores
+  // past this point, to ensure that all the stores in the critical
+  // section are visible to other CPUs before the lock is released,
+  // and that loads in the critical section occur strictly before
+  // the lock is released.
+  // On RISC-V, this emits a fence instruction.
+  __sync_synchronize();
+
+  // Release the lock, equivalent to lk->locked = 0.
+  // This code doesn't use a C assignment, since the C standard
+  // implies that an assignment might be implemented with
+  // multiple store instructions.
+  // On RISC-V, sync_lock_release turns into an atomic swap:
+  //   s1 = &lk->locked
+  //   amoswap.w zero, zero, (s1)
+  __sync_lock_release(&lk->locked);
+
+}
+
+int
+holding_mutex(struct thread_mutex *lk)
+{
+  int r;
+  r = (lk->locked && lk->pid == getpid());
+  return r;
+}
diff --git a/user/thread_mutex_lock.h b/user/thread_mutex_lock.h
new file mode 100644
index 0000000..7cf9b2b
--- /dev/null
+++ b/user/thread_mutex_lock.h
@@ -0,0 +1,11 @@
+#include "kernel/types.h"
+
+struct thread_mutex {
+
+  uint8 locked;      // Is the lock held?
+
+  // For debugging:
+  char *name;        // Name of lock.
+
+  uint8 pid; 
+};
\ No newline at end of file
diff --git a/user/thread_spinlock.c b/user/thread_spinlock.c
new file mode 100644
index 0000000..cb3bbe6
--- /dev/null
+++ b/user/thread_spinlock.c
@@ -0,0 +1,64 @@
+#include "user/thread_spinlock.h"
+
+void
+thread_spin_init(struct thread_spinlock *lk, char *name)
+{
+  lk->name = name;
+  lk->locked = 0;
+  lk->pid = 0;
+}
+
+// Acquire the lock.
+// Loops (spins) until the lock is acquired.
+void
+thread_spin_lock(struct thread_spinlock *lk)
+{
+
+  // On RISC-V, sync_lock_test_and_set turns into an atomic swap:
+  //   a5 = 1
+  //   s1 = &lk->locked
+  //   amoswap.w.aq a5, a5, (s1)
+  while(__sync_lock_test_and_set(&lk->locked, 1) != 0)
+    ;
+
+  // Tell the C compiler and the processor to not move loads or stores
+  // past this point, to ensure that the critical section's memory
+  // references happen strictly after the lock is acquired.
+  // On RISC-V, this emits a fence instruction.
+  __sync_synchronize();
+
+  lk->pid = getpid();
+}
+
+// Release the lock.
+void
+thread_spin_unlock(struct thread_spinlock *lk)
+{
+
+  lk -> pid = 0;
+  // Tell the C compiler and the CPU to not move loads or stores
+  // past this point, to ensure that all the stores in the critical
+  // section are visible to other CPUs before the lock is released,
+  // and that loads in the critical section occur strictly before
+  // the lock is released.
+  // On RISC-V, this emits a fence instruction.
+  __sync_synchronize();
+
+  // Release the lock, equivalent to lk->locked = 0.
+  // This code doesn't use a C assignment, since the C standard
+  // implies that an assignment might be implemented with
+  // multiple store instructions.
+  // On RISC-V, sync_lock_release turns into an atomic swap:
+  //   s1 = &lk->locked
+  //   amoswap.w zero, zero, (s1)
+  __sync_lock_release(&lk->locked);
+
+}
+
+int
+holding_spinlock(struct thread_spinlock *lk)
+{
+  int r;
+  r = (lk->locked && lk->pid == getpid());
+  return r;
+}
diff --git a/user/thread_spinlock.h b/user/thread_spinlock.h
new file mode 100644
index 0000000..3afe884
--- /dev/null
+++ b/user/thread_spinlock.h
@@ -0,0 +1,12 @@
+// Mutual exclusion lock.
+#include "kernel/types.h"
+
+struct thread_spinlock {
+
+  uint8 locked;      // Is the lock held?
+
+  // For debugging:
+  char *name;        // Name of lock.
+
+  uint8 pid; 
+};
\ No newline at end of file
diff --git a/user/threads.c b/user/threads.c
new file mode 100644
index 0000000..01a269e
--- /dev/null
+++ b/user/threads.c
@@ -0,0 +1,75 @@
+#include "kernel/types.h"
+#include "kernel/stat.h"
+#include "user/user.h"
+#include "user/thread_spinlock.c"
+#include "user/thread_mutex_lock.c"
+
+struct balance {
+    char name[32];
+    int amount;
+};
+
+volatile int total_balance = 0;
+
+struct thread_spinlock lock;
+
+struct thread_mutex mlock;
+
+volatile unsigned int delay (unsigned int d) {
+   unsigned int i;
+   for (i = 0; i < d; i++) {
+       __asm volatile( "nop" ::: );
+   }
+
+   return i;
+}
+
+void do_work(void *arg){
+    int i;
+    int old;
+
+    thread_mutex_lock(&mlock);
+    struct balance *b = (struct balance*) arg;
+    printf( "Starting do_work: s:%s\n", b->name);
+    thread_mutex_unlock(&mlock);
+
+    for (i = 0; i < b->amount; i++) {
+        // lock and mlock will be implemented by you.
+        //thread_spin_lock(&lock);
+        thread_mutex_lock(&mlock);
+        old = total_balance;
+        delay(100000);
+	 // if(old != total_balance)  printf("we will miss an update. old: %d total_balance: %d\n", old, total_balance);
+        total_balance = old + 1;
+        //thread_spin_unlock(&lock);
+        thread_mutex_unlock(&mlock);
+    }
+
+    printf( "Done s:%x\n", b->name);
+
+    thread_exit();
+    return;
+}
+
+int main(int argc, char *argv[]) {
+
+  struct balance b1 = {"b1", 3200};
+  struct balance b2 = {"b2", 2800};
+
+  void *s1, *s2;
+  int thread1, thread2, r1, r2;
+
+  s1 = malloc(4096); // 4096 is the PGSIZE defined in kernel/riscv.h
+  s2 = malloc(4096);
+
+  thread1 = thread_create(do_work, (void*)&b1, s1);
+  thread2 = thread_create(do_work, (void*)&b2, s2);
+
+  r1 = thread_join(thread1);
+  r2 = thread_join(thread2);
+
+  printf("Threads finished: (%d):%d, (%d):%d, shared balance:%d\n",
+      thread1, r1, thread2, r2, total_balance);
+
+  exit(0);
+}
\ No newline at end of file
diff --git a/user/user.h b/user/user.h
index 4d398d5..edc219b 100644
--- a/user/user.h
+++ b/user/user.h
@@ -39,3 +39,8 @@ void free(void*);
 int atoi(const char*);
 int memcmp(const void *, const void *, uint);
 void *memcpy(void *, const void *, uint);
+int thread_create(void (*)(void*), void *, void *);
+int thread_join(int);
+void thread_exit(void);
+int thread_sleep(uint8*);
+int thread_wakeup(int);
diff --git a/user/usys.pl b/user/usys.pl
index 01e426e..b930a30 100755
--- a/user/usys.pl
+++ b/user/usys.pl
@@ -36,3 +36,8 @@ entry("getpid");
 entry("sbrk");
 entry("sleep");
 entry("uptime");
+entry("thread_create");
+entry("thread_join");
+entry("thread_exit");
+entry("thread_sleep");
+entry("thread_wakeup");
\ No newline at end of file
